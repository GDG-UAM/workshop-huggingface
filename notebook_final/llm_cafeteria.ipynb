{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vamos a hacer un fine-tuning de un modelo muy simple text-to-text con datos de la cafetería de la UAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T5 (Text-to-Text Transfer Transformer) es un modelo preentrenado por Google que ya ha sido entrenado en miles de millones de frases (Wikipedia, libros, páginas web…)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La parte de aprender a entender el lenguaje y a generar texto coherente ya está definida en los parámetros de este modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/en/model_doc/t5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué es el fine-tuning entonces?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La idea principal es enseñarle al modelo una \"nueva materia\", donde busca patrones en un dataset para \"aprender\" a responder preguntas similares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podríamos decir que el modelo sabe \"hablar\" pero necesita aprender sobre \"qué hablar\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"fine_tuning.png\" alt=\"FT\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Como hacemos el fine-tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos modelos están entrenados en redes neuronales, que naturalmente no entienden texto, por lo que hay que tokenizar el texto, es decir, convertirlos en números que pueden entender las redes neuronales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\"¿Cuánto cuesta el café?\" → [1432, 209, 8743, 22, 1567, 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"token.png\" alt=\"tk\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tokenizado nuestro dataset, ya se puede empezar el fine-tuning del modelo. La idea general es que el modelo aprende las relaciones que hay entre las preguntas y respuestas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas relaciones se van grabando en los millones de parámetros que tiene, son pesos internos que minimizan el error de lo que genera y lo que debría generar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Cómo genera una respuesta?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando luego le haces una pregunta nueva como:\n",
    "```markdown\n",
    "¿Hay opciones sin gluten?\n",
    "```\n",
    "El modelo:\n",
    "\n",
    "- Tokeniza tu texto.\n",
    "- Pasa los tokens por el transformer.\n",
    "- Genera tokens de salida uno a uno, prediciendo la palabra siguiente más probable.\n",
    "- Decodifica esos tokens a texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que nos da un resultado como:\n",
    "```markdown\n",
    "\"Sí, tenemos galletas y bocadillos sin gluten.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué son los transformers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"transformers.png\" alt=\"Transformers\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/en/index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La librería transofrmers es la clave de todo este trabajo, que se basan en un concepto llamado atención (attention)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La idea es que el modelo no procesa las frases palabra por palabra en orden,\n",
    "sino que aprende a prestar atención a las partes más relevantes de la entrada para cada palabra que genera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es decir:\n",
    "\n",
    "Para responder `“1,20€”` a `“¿Cuánto cuesta el café con leche?”`,\n",
    "el modelo aprende que la palabra `“cuesta”` y `“café con leche”` son las claves,\n",
    "y no necesita `“¿”` o `“el”`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta forma los trasnformers manejan mejor el lenguaje natural en estos casos, ya que se se centran en manejar el contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/GDG-UAM/workshop-huggingface.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "!pip3 install \"transformers[torch]\" sentencepiece datasets pandas accelerate evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- json → para leer tu dataset en formato JSON.\n",
    "- torch → base de PyTorch, necesario para el modelo y entrenamiento.\n",
    "- T5ForConditionalGeneration → modelo T5 preentrenado para generación de texto.\n",
    "- T5Tokenizer → convierte texto en tokens (números que entiende el modelo).\n",
    "- Trainer y TrainingArguments → sistema de entrenamiento de Hugging Face, facilita mucho el proceso.\n",
    "- Dataset → estructura de datos personalizada de PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargamos nuestro dataset\n",
    "Usaremos cafeteria.json que tiene datos sobre el menú de todas las cafeterías de la UAM.\n",
    "Contiene pares de pregunta y respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar archivo cafeteria.json\n",
    "with open(\"cafeteria.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Verificar primeras entradas\n",
    "data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creamos la clase de CafeteriaDataset de PyTorch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch es un framework de deep learning, mientras Dataset es una clase abstracta de la que se hereda para crear datasets personalizados, permitiendo que el modelo acceda a datos de forma eficiente durante el entrenamiento (como en batches)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertimos las preguntas y respuestas en tensores numéricos que T5 pueda procesar.\n",
    "Esto se hace con un tokenizador, que traduce texto a secuencias de números."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CafeteriaDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input=64, max_output=64):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input = max_input\n",
    "        self.max_output = max_output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.data[idx][\"question\"]\n",
    "        answer = self.data[idx][\"answer\"]\n",
    "\n",
    "        # Tokenizar input (pregunta)\n",
    "        input_enc = self.tokenizer(\n",
    "            \"Pregunta: \" + question,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_input,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Tokenizar target (respuesta)\n",
    "        target_enc = self.tokenizer(\n",
    "            answer,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_output,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_enc[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": input_enc[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": target_enc[\"input_ids\"].squeeze(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- input_ids → tokens de la pregunta (entrada al modelo).\n",
    "- labels → tokens de la respuesta (lo que el modelo debe aprender a generar).\n",
    "- attention_mask → indica qué partes son reales y cuáles son padding (relleno).\n",
    "- El modelo aprende a generar la respuesta correcta dada una pregunta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparamos el modelo y tokenizamos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora debemos elegir un modelo exitente, en eeste caso elegimos t5-small, es un modelo “text-to-text”: convierte texto de entrada en texto de salida.\n",
    "Aquí lo cargamos junto con su tokenizador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- t5-small → versión ligera de T5 (más rápida para demos).\n",
    "- tokenizer → convierte texto a tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ususarios de Apple se utiliza MPS, si no se utiliza la cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detectar GPU Apple (MPS) o usar CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparamos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CafeteriaDataset(data, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuramos el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=2,  # pequeño para demo\n",
    "    num_train_epochs=10,\n",
    "    logging_steps=5,\n",
    "    save_total_limit=1,\n",
    "    remove_unused_columns=False,  # importante para T5\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- num_train_epochs=10 → el modelo pasa 10 veces por todo el dataset.\n",
    "- batch_size=2 → número de ejemplos por paso (puedes aumentarlo si tienes más RAM).\n",
    "- Trainer → se encarga de todo: feed-forward, backpropagation, optimización, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ahora entrenamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./modelo_cafeteria_t5\")\n",
    "tokenizer.save_pretrained(\"./modelo_cafeteria_t5\")\n",
    "print(\"Modelo entrenado y guardado en ./modelo_cafeteria_t5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuestro modelo ya estaría listo y entrenado ahora solo haría falta probarlo de la siguiente forma:\n",
    "\n",
    "- Cargamos el modelo que se ha creado\n",
    "- Lo probamos con la siguiente función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_modelo_y_responder(model_path=\"./modelo_cafeteria_t5\"):\n",
    "    \"\"\"\n",
    "    Carga el modelo T5 entrenado y devuelve una función para responder preguntas.\n",
    "    \"\"\"\n",
    "    # Cargar modelo y tokenizer\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    def responder(pregunta):\n",
    "        input_enc = tokenizer(\n",
    "            \"Pregunta: \" + pregunta,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=64,\n",
    "        )\n",
    "        input_enc = {k: v.to(device) for k, v in input_enc.items()}\n",
    "        outputs = model.generate(\n",
    "            **input_enc,\n",
    "            max_new_tokens=64,\n",
    "            num_beams=2,\n",
    "            early_stopping=True,\n",
    "        )\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return responder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruebas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responder = cargar_modelo_y_responder()\n",
    "\n",
    "print(responder(\"¿Cuánto cuesta el café con leche?\"))\n",
    "print(responder(\"¿Cuál es el precio del bocadillo de calamares?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
